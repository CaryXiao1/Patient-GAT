{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0wKw1XbUbWG"
   },
   "source": [
    "# Graph Representation with Lab Data only in Node Features: Directed Mixed Edge Definition\n",
    "Implements & generates a patient graph representation using Demographics and Lab Variables. Lab Variables both influence patient similarity using DTW on each variable and are normalized/concatenated to node features (after cosine similarity for nodes is calculated). Uses tenfold Cross Validation to generate ten separate Data Objects, one for each iteration.\n",
    "\n",
    "'Mixed' combines both knn and edge threshold by only adding the top k nearest neighbors if their similarity is above a flat threshold. \n",
    "\n",
    "\n",
    "This notebook generates the data objects that represent the patient graph network. After running this file, run the notebooks found in 'src/Model Train and Eval/' on the specified root folder defined by 'root_path' that includes your models in the 'processed' subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets seed for Pytorch Lightning, sklearn, numpy, etc.\n",
    "SEED = 1\n",
    "\n",
    "# defines number of neighbors for mixed (n_neighbors with flat threshold)\n",
    "n_neighbors = 5\n",
    "min_threshold = 0.65\n",
    "\n",
    "# defines whether to use shortened lab time series data (6 values per patient) or regular (12 per patient)\n",
    "shortened = True\n",
    "\n",
    "# root path to process raw dataset\n",
    "root_path = '../../data/lab-oversampled-mixed5,0.65-noDTW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W2oDcGKBUbWL",
    "outputId": "85d4a588-179a-46c9-bfb5-ddd83d9d1b70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import os.path as osp\n",
    "from os.path import exists\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from dtw import *\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from torch_geometric.data import Dataset, download_url, Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "# torch geometric\n",
    "try: \n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    # Installing torch geometric packages with specific CUDA+PyTorch version. \n",
    "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details \n",
    "    TORCH = torch.__version__.split('+')[0]\n",
    "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
    "\n",
    "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-geometric \n",
    "    import torch_geometric\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data\n",
    "\n",
    "pl.seed_everything(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all STUDY_ID indices that are not shared by BOTH df1\n",
    "# and df2. function assumes STUDY_ID is the index value in both\n",
    "# DataFrames.\n",
    "# Returns: modified df1, modified df2. \n",
    "def removeNonIntersections(df1, df2):\n",
    "    df1_index = df1.index.unique()\n",
    "    # modify df2 based on df1's index\n",
    "    df2 = df2[df2.index.isin(df1_index)]\n",
    "    # modify df1 based on df2's new index\n",
    "    df2_index = df2.index.unique()\n",
    "    df1 = df1[df1.index.isin(df2_index)]\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# takes in variables DataFrame & separates it into the 3 separate \n",
    "# DataFrames for each variable, also resetting the index value.\n",
    "# Returns: separate (num_patient x 12) dataframes for gluc, ldl, \n",
    "# & hemo, respectively.\n",
    "def separateVars(vars_df):\n",
    "    gluc_df, ldl_df, hemo_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    gluc_row, ldl_row, hemo_row = [], [], [] # <- variables for storing the time series for a given patient\n",
    "    prev_index = vars_df.index[0]\n",
    "    for index, row in vars_df.iterrows():\n",
    "        if index == prev_index:\n",
    "            # add new points to each row\n",
    "            gluc_row += [row['glucose_lab']]\n",
    "            ldl_row += [row['ldl_lab']]\n",
    "            hemo_row += [row['hemo_lab']]\n",
    "        else: \n",
    "            # reset for new STUDY_ID, add each row to each respective DataFrame\n",
    "            prev_index = index\n",
    "            gluc_df = pd.concat([gluc_df, pd.DataFrame([gluc_row])], axis=0)\n",
    "            ldl_df = pd.concat([ldl_df, pd.DataFrame([ldl_row])], axis=0)\n",
    "            hemo_df = pd.concat([hemo_df, pd.DataFrame([hemo_row])], axis=0)\n",
    "            \n",
    "            gluc_row = [row['glucose_lab']]\n",
    "            ldl_row = [row['ldl_lab']]\n",
    "            hemo_row = [row['hemo_lab']]\n",
    "    # add last row to df\n",
    "    gluc_df = pd.concat([gluc_df, pd.DataFrame([gluc_row])], axis=0)\n",
    "    ldl_df = pd.concat([ldl_df, pd.DataFrame([ldl_row])], axis=0)\n",
    "    hemo_df = pd.concat([hemo_df, pd.DataFrame([hemo_row])], axis=0)\n",
    "    # reset indices for each df\n",
    "    gluc_df = gluc_df.reset_index(drop=True)\n",
    "    ldl_df = ldl_df.reset_index(drop=True)\n",
    "    hemo_df = hemo_df.reset_index(drop=True)\n",
    "    \n",
    "    return gluc_df, ldl_df, hemo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separates DataFrame into train, test, and validate dfs\n",
    "# based on SEED & user-defined proportion of train, test split.\n",
    "# Returns: train_df, test_df, val_df\n",
    "def train_test_val(df, train_size=0.7, test_size=0.2, SEED=0):\n",
    "    train_df, test_df = train_test_split(df, train_size=train_size, random_state=SEED)\n",
    "    test_df, val_df = train_test_split(test_df, train_size=(test_size / (1 - train_size)), random_state=SEED)\n",
    "    return train_df, test_df, val_df\n",
    "\n",
    "\n",
    "# runs train_test_val on a list of dfs, adding them to the output dictionary based \n",
    "# on their respective index's name (as a str) in name_list.\n",
    "# Returns: train/test/val dictionaries containing each df's result; specify the string \n",
    "# name for the df to get the respective df mask.\n",
    "def run_train_test_val(df_list, name_list, train_size=0.7, test_size=0.2, SEED=0):\n",
    "    train_splits = {}\n",
    "    test_splits = {}\n",
    "    val_splits = {}\n",
    "    for i in range(len(df_list)):\n",
    "        train_df, test_df, val_df = train_test_val(df_list[i], train_size=train_size, test_size=test_size, SEED=SEED)\n",
    "\n",
    "        train_splits[name_list[i]] = train_df\n",
    "        test_splits[name_list[i]] = test_df\n",
    "        val_splits[name_list[i]] = val_df\n",
    "    return train_splits, test_splits, val_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses SMOTE to oversample a given X_df and y_df based on SEED.\n",
    "# Returns: oversampled X_df, y_df.\n",
    "def oversampleSMOTE(X_df, y_df, SEED=0):\n",
    "    smote = SMOTE(random_state=SEED)\n",
    "    return smote.fit_resample(X_df, y_df)\n",
    "\n",
    "\n",
    "# uses SMOTE on a dictionary of X_dfs and one y_df.\n",
    "# Returns: dictionary of oversampled X_dfs, oversampled y_df\n",
    "def oversampleSMOTE_dict(dict_X_df, y_df, SEED=0):\n",
    "    oversamp_dict = {}\n",
    "    smote = SMOTE(random_state=SEED)\n",
    "    for key in dict_X_df:\n",
    "        temp_y_df = y_df.copy()\n",
    "        oversamp_dict[key], temp_y_df = smote.fit_resample(dict_X_df[key], temp_y_df)\n",
    "    \n",
    "    return oversamp_dict, temp_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in the train, test, & validate dicts & concatenates\n",
    "# them into a new dictionary featuring all nodes.\n",
    "# Returns: dictionary containing ALL x nodes, ALL y nodes, etc.\n",
    "def concatSplits(train_splits, test_splits, val_splits):\n",
    "    splits = {}\n",
    "    # note: function assumes that all dicts have the same keys\n",
    "    for key in train_splits:\n",
    "        splits[key] = pd.concat((train_splits[key], test_splits[key], val_splits[key]), ignore_index=True)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a df with time series data and returns a np 2d \n",
    "# array of the dtw distances, displaying a loading value\n",
    "# with label load_str. \n",
    "# Returns: (len(df) x len(df)) np array with dtw distances.\n",
    "def dtwDistance(df, load_str):\n",
    "    dtw_distance = np.zeros([len(df), len(df)])\n",
    "    x = 0\n",
    "    for i1, row1 in df.iterrows():\n",
    "        print(load_str + ':  ' + str(x) + ' / ' + str(len(df)), end='\\r')\n",
    "        for i2, row2 in df.iterrows():\n",
    "            # get all 3 variables & caclulate each dtw\n",
    "\n",
    "            if (i1 == i2):\n",
    "                break\n",
    "\n",
    "            val = dtw(row1.tolist(), row2.tolist(), keep_internals=True).distance\n",
    "            dtw_distance[i1][i2] = val\n",
    "            dtw_distance[i2][i1] = val\n",
    "        x += 1\n",
    "    print(load_str + ':  done!              ')\n",
    "    return dtw_distance\n",
    "    \n",
    "    \n",
    "# uses dtwDistance to calculate the distance similarity matrix\n",
    "# for a given df, displaying a loading value with label\n",
    "# load_str.\n",
    "# Returns: (len(df) x len(df)) np array with dtw similarities\n",
    "def dtwSimilarity(df, load_str):\n",
    "    dtw_distance = dtwDistance(df, load_str)\n",
    "    \n",
    "    # get max and min value from dtw_distance to perform min-max normalization\n",
    "    # Note: dtw_similarity is defined by 1 - (min-max normalization of dtw_distance)\n",
    "    min_arr = np.zeros(len(dtw_distance))\n",
    "    max_arr = np.zeros(len(dtw_distance))\n",
    "    for i in range(len(dtw_distance)):\n",
    "        row = np.delete(np.copy(dtw_distance[i]), i) # only considers non-major diagonal entries\n",
    "        min_arr[i] = np.amin(row)\n",
    "        max_arr[i] = np.amax(row)\n",
    "    max_value = np.amax(max_arr)\n",
    "    min_value = np.amin(min_arr)\n",
    "    \n",
    "    # set major diagonal to 0\n",
    "    dtw_similarity = 1 - ((dtw_distance - min_value) / (max_value - min_value))\n",
    "    for i in range(len(dtw_similarity)):\n",
    "        dtw_similarity[i][i] = 0\n",
    "    return dtw_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in df for node features and computes cosine\n",
    "# similarity matrix, showing a loading bar with label\n",
    "# load_str.\n",
    "# Returns: (len(df) x len(df)) cosine similarity np array\n",
    "def cosSimilarity(df, load_str):\n",
    "    cos_similarity = np.zeros([len(df), len(df)])\n",
    "    x = 0\n",
    "    for index, row1 in df.iterrows():\n",
    "        print(load_str + ':  ' + str(x) + ' / ' + str(len(df)), end='\\r')\n",
    "        row_similarities = np.zeros([1, len(df)])\n",
    "        row_s_index = 0\n",
    "        for i, row2 in df.iterrows():\n",
    "            if (index == i):\n",
    "                break\n",
    "            # convert row and r into np arrays\n",
    "            row1_arr = row1.to_numpy()\n",
    "            row2_arr = row2.to_numpy()\n",
    "            # calculate row similarity\n",
    "            dot_product = np.dot(row1_arr, row2_arr) / (np.linalg.norm(row1_arr) * np.linalg.norm(row2_arr))\n",
    "            cos_similarity[x][row_s_index] = dot_product\n",
    "            cos_similarity[row_s_index][x] = dot_product\n",
    "            row_s_index += 1\n",
    "        x += 1\n",
    "    print(load_str + ':  done!               ', end='\\r')\n",
    "    return cos_similarity\n",
    "\n",
    "\n",
    "# takes in similarity matrix and converts it to an edge\n",
    "# index based on a flat threshold.\n",
    "# Returns: 2 x (# of edges) edge index np array\n",
    "def toEdgeIndex_threshold(similarity, threshold=0.5):\n",
    "    edge_start = []\n",
    "    edge_end = []\n",
    "    for row in range(len(similarity)):\n",
    "        for col in range(len(similarity[row])):\n",
    "            if similarity[row][col] >= threshold and row != col:\n",
    "                # create 2 by 1 np array and concatenate to main one\n",
    "                edge_start += [row]\n",
    "                edge_end += [col]\n",
    "    return [edge_start, edge_end]\n",
    "\n",
    "\n",
    "# uses k-Nearest Neighbors with a minimum threshold to \n",
    "# define the edge index based on a set # of neighbors \n",
    "# per node and defined min_threshold.\n",
    "# Returns: 2 x (2 * num_neighbors * # of nodes) edge index np array\n",
    "def toEdgeIndex_mixed(similarity, num_neighbors=5, min_threshold=0.75):\n",
    "    edge_start, edge_end = [], []\n",
    "    for row in range(len(similarity)):    # num_neighbors, so a lot of nodes will have > num_neighbors neighbors)\n",
    "        # find top N nodes to form edges with\n",
    "        top_n_neighbors = np.argpartition(similarity[row], -num_neighbors)[-num_neighbors:]\n",
    "        for end in top_n_neighbors:\n",
    "            if similarity[row][end] >= min_threshold:\n",
    "                edge_start += [row]\n",
    "                edge_end   += [end]\n",
    "    return [edge_start, edge_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs min-max normalization on time series df.\n",
    "# Returns: normalized time series df.\n",
    "def minMaxNormalize(df):\n",
    "    min_arr = np.zeros(len(df.columns))\n",
    "    max_arr = np.zeros(len(df.columns))\n",
    "    for col in df:\n",
    "        min_arr[col] = df.loc[df[col].idxmin()][col]\n",
    "        max_arr[col] = df.loc[df[col].idxmax()][col]\n",
    "    min_value = np.amin(min_arr)\n",
    "    max_value = np.amax(max_arr)\n",
    "    return (df.select_dtypes(include=['float64']) - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a list of dfs, a list of names for the dfs, and \n",
    "# the train/test/val index arrays and creates dictionaries\n",
    "# for train/test/val.\n",
    "# Returns: train df dict, test df dict, val df dict\n",
    "def train_test_val_index(df_list, name_list, train_indices, test_indices, val_indices):\n",
    "    train_splits = {}\n",
    "    test_splits = {}\n",
    "    val_splits = {}\n",
    "    for i in range(len(df_list)):\n",
    "        train_df, test_df, val_df = df_list[i].iloc[train_indices], df_list[i].iloc[test_indices], df_list[i].iloc[val_indices]\n",
    "        train_splits[name_list[i]] = train_df\n",
    "        test_splits[name_list[i]] = test_df\n",
    "        val_splits[name_list[i]] = val_df\n",
    "    return train_splits, test_splits, val_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates train/test/val index arrays for tenfold\n",
    "# cross validation. \n",
    "# Returns: (10 x (train block size * size of block)) train index array, \n",
    "#          (10 x (test block size * size of block)) test index array, \n",
    "#          (10 x (val block size * size of block)) val index array, \n",
    "def generate_split_indices(node_len, k_fold=10, train_size=7, test_size=2, val_size=1, SEED=0):\n",
    "    np.random.seed(SEED)\n",
    "    indices = np.array(range(node_len))\n",
    "    # shuffle indices, pop extra indices into seperate array\n",
    "    np.random.shuffle(indices)\n",
    "    indices_remain = indices[-(node_len % k_fold):]\n",
    "    indices = indices[:-(node_len % k_fold)]\n",
    "    # double the length to simplify looping of block categories & split it into 10 (* 2) buckets\n",
    "    indices_double = np.concatenate((indices, indices))\n",
    "    ind_double_list = np.split(indices_double, k_fold * 2)\n",
    "    # concatenate remaining values onto first block\n",
    "    ind_double_list[0] = np.concatenate((ind_double_list[0], indices_remain))\n",
    "    \n",
    "    train_splits, test_splits, val_splits = [], [], []\n",
    "    for train_block_start in range(k_fold):\n",
    "        # generate the index arrays for the ith start\n",
    "        train_split = np.array(ind_double_list[train_block_start])\n",
    "        test_split = np.array(ind_double_list[train_block_start + train_size])\n",
    "        val_split = np.array(ind_double_list[train_block_start + train_size + test_size])\n",
    "        for i in range(1, train_size):\n",
    "            train_split = np.concatenate((train_split, ind_double_list[train_block_start + i]))\n",
    "        for i in range(1, test_size):\n",
    "            test_split = np.concatenate((test_split, ind_double_list[train_block_start + train_size + i]))\n",
    "        for i in range(1, val_size):\n",
    "            val_split = np.concatenate((val_split, ind_double_list[train_block_start + train_size + test_size + i]))\n",
    "        train_splits += [train_split]\n",
    "        test_splits += [test_split]\n",
    "        val_splits += [val_split]\n",
    "    return train_splits, test_splits, val_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates ten data objects for the TenfoldDataset that will\n",
    "# later be initialized with the specified root. Each data object\n",
    "# saved into the specified root directory will later be trained /\n",
    "# tested.\n",
    "# Returns: nothing but saves the 10 crossVal data objects in the\n",
    "# processed data folder in the root database. \n",
    "def tenfold_cross_val(root=None, train_num=7, test_num=2, val_num=1, SEED=0):\n",
    "    np.random.seed(SEED)\n",
    "    # type check: make sure train/test/block sizes are positive and add to 10, root is specified\n",
    "    if train_num <= 0 or test_num <= 0 or val_num <= 0 or train_num + test_num + val_num != 10:\n",
    "        raise Exception(\"Block sizes must all be positive and add to 10.\")\n",
    "    if root == None:\n",
    "        raise Exception(\"Please specify a root directory.\")\n",
    "\n",
    "    print('importing x, y, and lab variables, removing non-intersecting patients...')\n",
    "    x_df = pd.read_csv(root + '\\\\raw\\\\x_pd.csv', index_col=0)\n",
    "    y_df = pd.read_csv(root + '\\\\raw\\\\y_pd.csv', index_col=0)\n",
    "    \n",
    "    vars_path = root + '\\\\raw\\\\imputed_lab' + '_shortened.csv' if shortened else '.csv'\n",
    "    \n",
    "    vars_df = pd.read_csv(vars_path, index_col=1)\n",
    "    vars_df = vars_df.drop(columns=['Unnamed: 0'])\n",
    "    x_df, vars_df = removeNonIntersections(x_df, vars_df)\n",
    "    y_df, vars_df = removeNonIntersections(y_df, vars_df)\n",
    "\n",
    "    print('splitting variables into gluc, ldl, & hemo; resetting index and randomizing buckets...')\n",
    "    # separate variables into separate gluc, ldl, and hemo dfs; reset index values:\n",
    "    gluc_df, ldl_df, hemo_df = separateVars(vars_df)\n",
    "    x_df = x_df.reset_index(drop=True)\n",
    "    y_df = y_df.reset_index(drop=True)\n",
    "\n",
    "    # calculate train/test/val indices for tenfold CV\n",
    "    train_indices, test_indices, val_indices = generate_split_indices(len(x_df), SEED=SEED)\n",
    "    \n",
    "    \n",
    "    # specify block indices for cross validation:\n",
    "    indices = np.array(range(len(x_df)))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # calculate the data object for each train-test-val split in the tenfold CV:\n",
    "    for i in range(10):\n",
    "        print('\\n\\nstarting data calculation ' + str(i) + ':\\n----------')\n",
    "        \n",
    "        # check that file has not yet been created\n",
    "        if exists(root + '\\\\processed\\\\data_' + str(i) + '.pt'):\n",
    "            print('file already exists. Advancing to next block data generation...')\n",
    "            continue\n",
    "\n",
    "        # create separate dfs for train/test/val:\n",
    "        df_list = [x_df, y_df, gluc_df, ldl_df, hemo_df]\n",
    "        name_list = ['x', 'y', 'gluc', 'ldl', 'hemo']\n",
    "        train_splits, test_splits, val_splits = train_test_val_index(df_list, name_list, train_indices[i], \n",
    "                                                                     test_indices[i], val_indices[i])\n",
    "        # oversample training data\n",
    "        y_train = train_splits.pop('y')\n",
    "        train_splits, y_train = oversampleSMOTE_dict(train_splits, y_train, SEED=SEED)\n",
    "        train_splits['y'] = y_train\n",
    "\n",
    "        # create train/test/val masks, create overall dictionary of all nodes/features TODO: decompose this!\n",
    "        train_mask = [1] * len(train_splits['x']) + [0] * (len(test_splits['x']) + len(val_splits['x']))\n",
    "        test_mask = [0] * len(train_splits['x']) + [1] * len(test_splits['x']) + [0] * len(val_splits['x'])\n",
    "        val_mask = [0] * (len(train_splits['x']) + len(test_splits['x'])) + [1] * len(val_splits['x'])\n",
    "        splits = concatSplits(train_splits, test_splits, val_splits)\n",
    "        print('Number of nodes after oversampling:', len(splits['x']))\n",
    "\n",
    "#         # calculate DTW distance\n",
    "#         dtw_similarity_gluc = dtwSimilarity(splits['gluc'], 'Calculating DTW Similarity matrix for Glucose')\n",
    "#         dtw_similarity_ldl = dtwSimilarity(splits['ldl'], 'Calculating DTW Similarity matrix for LDL')\n",
    "#         dtw_similarity_hemo = dtwSimilarity(splits['hemo'], 'Calculating DTW Similarity matrix for Hemoglobin')\n",
    "        \n",
    "        \n",
    "        # min-max normalize lab data\n",
    "        splits['gluc'] = minMaxNormalize(splits['gluc'])\n",
    "        splits['ldl'] = minMaxNormalize(splits['ldl'])\n",
    "        splits['hemo'] = minMaxNormalize(splits['hemo'])\n",
    "\n",
    "        # append lab data to x (before calculating cosine similarity)\n",
    "        splits['x'] = pd.concat((splits['x'], splits['gluc'], splits['ldl'], splits['hemo']), axis=1, ignore_index=True)\n",
    "        cos_similarity = cosSimilarity(splits['x'], 'Calculting Cosine Similarity for non-lab metrics')\n",
    "\n",
    "        # calculate cosine similarity, general similarity, & generate edge index\n",
    "        similarity = cos_similarity # (1 - dtw_weight) * cos_similarity + dtw_weight * (dtw_similarity_gluc + dtw_similarity_ldl + dtw_similarity_hemo) / 3\n",
    "        edge_index = toEdgeIndex_mixed(similarity, num_neighbors=n_neighbors, min_threshold=min_threshold)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(x=torch.Tensor(splits['x'].to_numpy()), edge_index = torch.LongTensor(edge_index))\n",
    "        data.y = torch.tensor(splits['y']['SARCOPENIA'].to_numpy(), dtype=torch.long)\n",
    "        data.train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "        data.test_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
    "        data.val_mask = torch.tensor(val_mask, dtype=torch.bool)\n",
    "        data.num_classes = 2\n",
    "\n",
    "        torch.save(data, root + '\\\\processed\\\\' + f'data_{i}.pt')\n",
    "    print('\\n------------------------\\ndone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing x, y, and lab variables, removing non-intersecting patients...\n",
      "splitting variables into gluc, ldl, & hemo; resetting index and randomizing buckets...\n",
      "\n",
      "\n",
      "starting data calculation 0:\n",
      "----------\n",
      "Number of nodes after oversampling: 1260\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 1:\n",
      "----------\n",
      "Number of nodes after oversampling: 1252\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 2:\n",
      "----------\n",
      "Number of nodes after oversampling: 1244\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 3:\n",
      "----------\n",
      "Number of nodes after oversampling: 1258\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 4:\n",
      "----------\n",
      "Number of nodes after oversampling: 1234\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 5:\n",
      "----------\n",
      "Number of nodes after oversampling: 1242\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 6:\n",
      "----------\n",
      "Number of nodes after oversampling: 1240\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 7:\n",
      "----------\n",
      "Number of nodes after oversampling: 1264\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 8:\n",
      "----------\n",
      "Number of nodes after oversampling: 1272\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "\n",
      "starting data calculation 9:\n",
      "----------\n",
      "Number of nodes after oversampling: 1266\n",
      "Calculting Cosine Similarity for non-lab metrics:  done!               \n",
      "------------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "tenfold_cross_val(root=root_path, SEED=SEED)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GNN_overview.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
